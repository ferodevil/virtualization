* # 无线网卡桥接总结

## 背景介绍

* 无线场景下，虚拟机中通过nat方式与外界互联；

* 有线场景下，虚拟机通过bridge方式与外界互联；

* 无线网卡处于managed模式；

nat网络有如下两点限制：

1. 虚拟机内部ip为私有ip，最终与外界通信均通过host中ip来完成，即外部无法通过此私有ip与虚机内服务完成通信；
2. nat将host当成转发路由器，效率较为低下；

本文的目的即是在保证虚拟机网络正常的情况下避免如上限制，是否有与有线场景下同样的实现方案。

## 问题介绍

* 1. 最直接的想法是能否复用有线的实现方式，直接将wlan0加入到虚拟网桥，如能正常工作就彻底搞定；结果加入网桥的时候提示Operation not supported：

```
root:~# brctl show
bridge name  bridge id          STP enabled interfaces
br0          8000.58696cff3bba  no          eth0
                                            vnet0
root:~# brctl addif br0 wlan0
can't add wlan0 to bridge br0: Operation not supported
```

* 2. 如无法与有线共用方案，是否有其他方案可供选择？

## 原因分析
- **kernel限制**

如下错误提示，最直接原因为kernel网桥模块对此进行了限制，不让无线网卡加入到网桥中，通过查看内核源码可知：

```
...
if ((wdev->iftype == NL80211_IFTYPE_STATION ||
    wdev->iftype == NL80211_IFTYPE_ADHOC) && !wdev->use_4addr)
      dev->priv_flags |= IFF_DONT_BRIDGE;
...
/* No bridging devices that dislike that (e.g. wireless) */
if (dev->priv_flags & IFF_DONT_BRIDGE)
    return -EOPNOTSUPP;
```

从源码中可知，当未启用4addr的同时，无线网卡处于managed模式或者adhoc模式的情况下，禁止将无线网卡加入到网桥设备，这样又涉及到802.11协议以及无线网卡的各种模式问题；

- **802.11协议介绍**

针对此问题，我们需要大致了解无线网卡各种模式以及802.11协议，其中模式我们只需要了解如下两种即可：

1. master模式，即无线ap所处模式，此模式下无线网卡定期广播beacon帧，此帧中含有SSID等必要信息，供各种终端扫描连接；
2. managed模式，即正常终端所处模式，此模式下通过扫描出来的SSID网络，然后选择与特定的无线网络进行连接；

802.11协议区分为3addr模式与4addr模式，详见附录：

> 3addr模式，大部分终端都是通过此模式来与ap进行通信，一般情况下ap也处于此模式下，如果主动将终端无线网卡设置为4addr模式，ap将忽略此报文，即终端无法通过ap连接外部网络；  
> 4addr模式，通过网上资料，此模式主要用户ap直接级联使用，不过在IDV终端上在managed模式下也可以设置为4addr模式，不确定此中情况是否有其他作用；    

- **初步尝试**

网络上将无线网卡加入网桥的方案，基本上都是将无线网卡直接设置（通过hostapd）成master模式，或者通过iw将无线网卡设置为4addr模式来实现；

```
iw dev <devname> set 4addr <on|off>
    Set interface 4addr (WDS) mode.
```

而将终端变成ap或者通过4addr级联均不是我们想要的结果。

- **总结**

综上所述，大部分无线场景下，均使用3addr模式，而4addr模式主要用户ap级联的场景下，对应为WDS模式，与我们需求相悖，不考虑；而在3addr模式下，终端与ap认证通过后，本质上认证的为终端的mac地址，同时3addr只有一个mac地址保留给终端，即不是本终端mac相关的报文均会被ap所丢弃，所以kernel主动将其限制住，这种情况下无法加入网桥；  

剩下能选择的方案是能否通过mac欺骗的方式让ap认为虚机中发出的报文即是终端所发出的即可；  
1. 最暴力的做法就是通过PF SOCKET抓出所有的包并识别出虚机所属的包然后将mac修改掉再通过wlan0发出去，这样基本不靠谱，放弃；  
2. 是否存在相关机制能够实现这个功能，而linux本身已实现多种虚拟网卡，下面从这方面入手分析是否存在。

[内核限制](https://github.com/torvalds/linux/commit/ad4bb6f8883a13bb0f65b194dae36c62a02ac779)

## 方案分析

### 1.  **虚拟机网络设置**
首先来介绍一下虚拟机的网络数据流，不管最终以何种方式实现，均需要能让虚拟机使用才行；

```
-netdev tap,fd=30,id=hostnet0,vhost=on,vhostfd=32                                      （host角度）
-device virtio-net-pci,netdev=hostnet0,id=net0,mac=52:54:00:68:00:20,bus=pci.0,addr=0x3（虚拟机角度）
```

如上所述，我们对qemu通过如上配置即可实现虚拟机与外部网络互通的效果，其中-device目的为给虚拟机模拟一个网卡设备，此处使用virtio网卡，虚拟机内部发送网络数据最终都需要通过此模拟的网卡设备进行处理；同时-netdev为针对host的配置，目的为将模拟的网卡设备中的数据送到真实的物理网卡上，并将其转发出去，此处有两点需要了解，vhost=on表示使用vhost-net网卡，与virtio-net的区别为其数据收发在内核中实现，虚拟机中发送的数据报文直接在内核中收发，不再需要经过qemu；tap，fd=30表示的为linux中的虚拟网卡tap设备，vhost将网络报文转发给此tap网卡，再又tap虚拟网卡通过虚拟网桥或者其他的方式转发到真实存在物理网卡，最终实现与外界互联。
>桥接模式：  
>数据发送：虚拟机->vhost->tap设备->虚拟网卡->网桥->物理网卡  
>数据接收：相反流程  
>
>ipvtap/macvtap模式：  
>数据发送：虚拟机->vhost->tap设备->虚拟网卡->物理网卡  
>数据接收：相反流程  

从上述分析，qemu对网卡的设置可以一直保持一致，更换不同模式主要对应后端linux实现不一样，如bridge或者nat只是host端tap网络转发实现不一样而已。

[qemu配置虚拟机网络](https://github.com/shimachao/blog/blob/master/%E8%99%9A%E6%8B%9F%E5%8C%96%E6%8A%80%E6%9C%AF/%E3%80%90%E8%AF%91%E3%80%91%E9%85%8D%E7%BD%AE%E5%AE%A2%E6%88%B7%E6%9C%BA%E7%BD%91%E7%BB%9C.md)

### 2.  **linux虚拟网卡**

下面来介绍一下linux实现的一些虚拟网卡，包括其大致工作原理。  
linux内核中存在一个网络设备管理层，处于网络设备驱动和协议栈之间，对于驱动来说不需要了解协议栈的细节，同样对于协议栈来说也不需要了解设备驱动细节；  
比如对于一个物理网卡eth0，其连接内核协议栈以及外部物理网络，从物理网络收到的数据，会转发给协议栈，最终交由应用进行处理；同样应用发送的数据会经由协议栈并转发给驱动层，并最终转发到外部物理网络；  
那对于一个虚拟网卡呢？首先它同样归网络设备管理层进行管理，对其来说，其与物理网卡并无区别，都能配置ip，其接收到的数据也都会转发给协议栈，而协议栈过来的数据也都交由其进行转发，只不过物理网卡直接转发到外部物理网络，而虚拟网卡怎么转发依赖内核实现。  

* 物理网卡  
  ![eth网卡](/assets/eth.png)

* tap/tun设备  
  ![tun设备](/assets/tun.png)
>普通的网卡通过网线收发数据，而tun设备通过文件收发数据，对应linux中字符设备，所有对这个文件的写操作会通过tun设备转换成数据包转发给内核，同样内核发送一个数据包给tun设备时，通过读这个文件可以获取其内容。

  ![tun设备使用示例](/assets/tun-example.png)
>如上图所示，通过tun设备完成vpn隧道功能。

tun设备与tap设备区别：

> tun设备收发的为IP层数据包，只能工作在IP层，无法与物理网卡做bridge，但可以通过三层交换如ip\_vorward与物理网卡联通；  
> tap设备收发的为mac层数据包，可以与物理网卡做bridge，支持mac层广播。

* macvlan  
  ![macvlan图示1](/assets/macvlan.png)  
>如上图所示，macvlan本质上为将一物理网卡虚拟出多个虚拟网卡，且具有独立的mac地址。  

  ![macvlan图示2](/assets/macvlan1.png)
>如上图所示，为具体使用示例，图中3个vm具有独立的mac与ip地址。   

根据macvlan子接口通信方式，macvlan区分为四种网络模式：

1.  vepa（virtual ethernet port aggregator）模式  
  ![macvlan vepa](/assets/macvlan-vepa.png)  

默认模式，在这种模式下，子接口通信需要外部支持802.1qbg/VPEA功能交换机支持，经外部转发再绕回来；  
2.  private模式  
  ![macvlan private](/assets/macvlan-private.png) 

在这种模式下，同一主接口下子接口彼此隔离，不能通信，即使从外部交换机转发，也会被丢弃；  
3.  bridge模式  
  ![macvlan bridge](/assets/macvlan-bridge.png) 

在这种模式下，与linux bridge功能类似，区别在于此模式每个mac地址的已知的，不用学习，在此模式下，子接口可以直接通信；  
4.  passthrough模式  
  ![macvlan passthrough](/assets/macvlan-passthrough.png)   

在这种模式下，只允许单个子接口连接主接口，且必须设置为混杂模式，一般用于子接口桥接和创建VLAN子接口的场景；  
> 802.1qbg/VPEA功能简单的说需要交换机支持hairpin功能，即将数据包从某一接口接收后再从此接口发出去 

* macvtap  
  ![macvtap设备](/assets/macvtap.png)

macvtap将macvlan与tap设备综合在一起，即macvlan将收到的包交给协议栈，而macvtap将收到的包交给/dev/tapX文件，通过这个文件，完成与用户态进程的直接通信；  
libvirt启动macvtap网卡如下所示：
```
...
<devices>
  <interface type='direct' trustGuestRxFilters='yes'>
    <source dev='eth0'/>
    <mac address='52:54:00:5d:c7:9e'/>
    <boot order='1'/>
    <rom bar='off'/>
  </interface>
</devices>
...
```
>直接命令行启动macvtap网络设备  
> 1.  启动虚机   
>创建macvtap虚拟网卡：ip link add link eth0 name macvtap0 address ${MACaddr} type macvtap mode bridge    
>获取tap设备index：TAPNUM=$(< /sys/class/net/macvtap0/ifindex)  
>获取tap设备mac地址：MACaddr=$(< /sys/class/net/macvtap0/address)  
>启动虚拟机 ： qemu-system-x86_64 -name MacVTap0 -localtime -curses \  
       -m ${MEM} -enable-kvm \
       -monitor unix:/src3/KVM/network-11586/MonSock,server,nowait \  
       -netdev tap,fd=3,id=hostnet0,vhost=on \  
       -net nic,vlan=0,netdev=hostnet0,macaddr=${MACaddr},model=virtio \  
       -drive index=0,media=disk,if=virtio,file=../img/MacVLan0.img 3<>/dev/tap${TAPNUM}  
> 2.  关闭虚拟机
> 清理qemu进程
> 关闭macvtap网卡：  
>if [ -d /proc/sys/net/ipv4/conf/macvtap0 ]; then    
>    sudo ip link set dev macvtap0 down  
>    sudo ip link delete macvtap0  
>fi  

* ipvlan  

  ![ipvlan设备](/assets/ipvlan1.png)

ipvlan与macvlan类似，区别在于ipvlan虚拟出的网卡具有相同的mac地址，且与物理接口共用通一个mac地址，但同样可以配置不同的ip地址；

1.  L2模式：  
与macvlan的bridge模式类似，父接口作为交换机来转发子接口数据；同一网络子接口通过父接口来转发数据，而如果想要发送到其他网络，报文通过父接口路由转发出去。

2.  L3模式：  
ipvlan类似路由器功能，在各个虚拟网络和主机直接进行不同报文路由转发工作，L3模式下虚拟接口不会接收到多播或者广播报文（ipvlan接口会有NOARP标志），所有arp过程或者多播过程都是在父接口中完成；

3. L3s模式  
与L3模式类似

> 此时外部网络不知道ipvlan虚拟出来的网络，需要手动配置好对应的路由规则，才能被外部网络之间访问；  
> 使用dhcp分配ip时，一般通过mac地址作为机器的标识，此种情况下就无法通过dhcp获取ip，因此需要配置唯一的clientid作为机器的标识，才能正常完成dhcp；或者只能通过静态ip的方式来配置ip。

[kernel ipvlan使用说明](https://www.kernel.org/doc/Documentation/networking/ipvlan.txt)

* ipvtap  
ipvtap对于ipvlan，与macvtap对于macvlan类似，均为收到报文后不送往协议栈，而是通过/dev/tapX设备来与用户态程序进行交互。  

### 3.  **选型与实验**
通过如上分析，我们可以选择ipvtap作为wlan0的tap网络实现，即可以满足3addr模式下1个mac地址的限制，也能满足独立配置ip，外部网络可以通过此ip与虚拟机进行直接通信的需求；  
如下分ipvlan与ipvtap分别对此需求进行验证：  
* ipvlan  
ipvlan通过linux中namespace来进行验证：  
```
ip netns add net1  
ip link add ipv1 link wlan0 type ipvlan mode l2
ip link set ipv1 netns net1  
ip netns exec net1 ip addr add *.*.*.*/24 dev ipv1  
ip netns exec net1 ip link set dev ipv1 up
ip netns exec net1 ip route add default gw *.*.*.*  
ip netns exec net1 ping www.baidu.com    
```
最终ping外界网络正常即表示ipvlan使用成功，即表明通过此种方式确实能实现无线网络的桥接，接下来的步骤就是将其复用在虚机环境下。  
* ipvtap  

1. 创建ipvtap：  
```
ip link add ipvtap1 link wlan0 type ipvtap
ip link set dev ipvtap1 up
```
创建成功后在/dev目录下会生成/dev/tapX设备文件，qemu/vhost-net通过此设备文件与虚拟出来的网卡进行交互。  

2. 启动qemu虚机  
```
#!/bin/bash
/usr/bin/qemu-system-x86_64 \
-name pq \
-cpu host,hv_time \
-machine pc,accel=kvm,usb=off \
-m 1024 \
-smp 4,sockets=1,cores=4,threads=1 \
-drive file=/opt/lessons/virtual_win10_64_pro.img,if=none,id=drive-ide0-0-1,format=qcow2,cache=writeback \
-device ide-hd,bus=ide.0,unit=1,drive=drive-ide0-0-1,id=ide0-0-1 \
-netdev tap,fd=30,id=hostnet0,vhost=on,vhostfd=32 \
-device virtio-net-pci,netdev=hostnet0,id=net0,mac=18:1d:ea:21:df:40,bus=pci.0,addr=0x3 \
-device intel-hda,id=sound0 \
-device hda-micro,id=sound0-codec0,bus=sound0.0,cad=0 \
-boot order=c,menu=on,splash-time=30000,strict=on \
-vnc 0.0.0.0:1 \
-spice port=5900,addr=0.0.0.0,disable-ticketing,seamless-migration=on \
-device qxl-vga,id=video0,ram_size=134217728,vram_size=67108864,vram64_size_mb=0 \
-device piix3-usb-uhci,id=usb,bus=pci.0,addr=0x1.0x2 \
-device usb-tablet,id=input0,bus=usb.0,port=1 30<>/dev/tap9 32<>/dev/vhost-net  
```   

首先命令行通过描述符重定向的方式验证，同样也可以在libvirt中通过netlink方式完善对ipvtap的支持。   
3. 在虚机中配置合适的ip地址，并观察网络通路是否正常，不幸的是，此时网络无法正常，需进一步分析原因；  
4. 最终分析结果为数据包在交给虚机对应tap设备的时候无法找到具体的地址，所以在ipvlan层被直接丢弃；归根到底ipvlan基于ip来转发数据，为什么host验证ipvlan正常，因为其会通过ip addr add xxx来完成对其地址的绑定；而在ipvtap方案下ip为在虚拟机Windows中设置，而这个无法被ipvlan感知，所以后续所有送给虚机的数据均在ipvlan中被丢弃；  

分析ipvlan模块代码，其在初始化过程中会注册一个监听网卡事件的钩子函数，具体如下所示，检测到host中网卡up/down事件后，对应内核为NETDEV_UP/NETDEV_DOWN事件，会触发对应的钩子函数完成对ip地址的保存；而虚机的网卡为virtio网卡，在host无法感知其up/down事件，所以此钩子函数也就无法触发，最终效果即为所有数据都在ipvlan中被丢弃。
```
static struct notifier_block ipvlan_addr4_notifier_block __read_mostly = {
	.notifier_call = ipvlan_addr4_event,
};
static int __init ipvlan_init_module(void)
{
	int err;

	...
    register_inetaddr_notifier(&ipvlan_addr4_notifier_block);
    ...
}
```
  
```
[  572.724450]  ipvlan_ht_addr_add+0x74/0x190 [ipvlan]
[  572.724455]  ipvlan_open+0x58/0x90 [ipvlan]
[  572.724461]  __dev_open+0xd7/0x170
[  572.724464]  __dev_change_flags+0x17e/0x1d0
[  572.724468]  dev_change_flags+0x29/0x60
[  572.724472]  devinet_ioctl+0x5e2/0x710
[  572.724476]  inet_ioctl+0x16e/0x250
[  572.724480]  ? inet_ioctl+0x16e/0x250
[  572.724483]  ? dev_get_by_name_rcu+0x74/0xa0
[  572.724485]  ? dev_get_by_name_rcu+0x74/0xa0
[  572.724490]  sock_do_ioctl+0x4d/0x150
[  572.724494]  ? inet_stream_connect+0x60/0x60
[  572.724497]  ? sock_do_ioctl+0x4d/0x150
[  572.724501]  sock_ioctl+0x1e2/0x330
[  572.724505]  ? sock_ioctl+0x1e2/0x330
[  572.724511]  do_vfs_ioctl+0xa9/0x640
[  572.724515]  ? dlci_ioctl_set+0x30/0x30
[  572.724518]  ? do_vfs_ioctl+0xa9/0x640
[  572.724522]  ? call_rcu_sched+0x17/0x20
[  572.724526]  ? __fput+0x17b/0x220
[  572.724530]  ksys_ioctl+0x75/0x80
[  572.724534]  __x64_sys_ioctl+0x1a/0x20
[  572.724539]  do_syscall_64+0x5a/0x120
[  572.724542]  entry_SYSCALL_64_after_hwframe+0x44/0xa9
```

### 4.  **剩余产品化问题**
- 默认ipvlan为L3模式，此时arp报文无法送到虚拟机中，此时外界需要手动配置路由表才能与虚机互通，且目前看来无对外设置，需要修改内核实现，将其默认修改为L2模式； 
> 直接修改源码即可  

- 虚机中设置/修改/删除静态ip需要能有途径反馈到ipvlan中，并执行如上所述的钩子函数；  
> 修改源码，增加机制即可

- dhcp问题，dhcp通过客户端mac地址分配ip，此时由于虚机的mac与wlan0 mac相同，无法通过正常dhcp方式获取ip。 
> - 在某些dhcp server中会根据clientid标识进行不同的动作，或者分配一个新的ip，或者判断其与mac地址是否一致，不一致则不会发送replay消息，即如果依赖clientid来获取多个ip的话需要依赖dhcp server实现，方案限制大；  
> - 静态设置ip，且需要设置成与host同一网段，如果ap再把此ip分配出去会存在ip冲突的风险，方案限制也大；  
> - 伪造一个假mac来获取ip，依赖dhcp replay为广播报文(dhcp replay可为单播或/广播)，目前验证在有线环境下可正常工作，在无线环境下无法正常工作，方案不可行；  
> - 利用dhcp协议decline特性，第1次获取ip的时候先占着ip，然后发decline报文回去，这时候server会再给一个ip，再把这第2个ip占着，后续需要对这两个ip都进行续租工作，方案限制就是ip会不停的变化。 
> dhcp server decline行为？  

[ipvlan代码分析参考](http://www.tecyle.com/2017/08/27/ipvlan-%e8%99%9a%e6%8b%9f-%e7%bd%91%e5%8d%a1/)  

[dhcp client id](https://www.net.princeton.edu/announcements/dhcp-cliid-must-match-chaddr.html) 

[dhcp协议](https://blog.csdn.net/zzd_zzd/article/details/88372014) 

[dhclient源码](http://www.linuxfromscratch.org/blfs/view/svn/basicnet/dhcp.html) 

[dhcp server decline](http://www.callevanetworks.com/understanding-infoblox-isc-dhcp-and-abandoned-leases/)

## 附录

### 1.  802.11 mac帧格式

![](https://github.com/ferodevil/virtualization/raw/master/assets/802.11帧格式.png)

根据802.11 MAC帧的传输方式，可以将MAC帧的地址结构分为三地址结构和四地址结构。其中，AP与STA之间传输的MAC帧采用三地址结构，AP与AP之间传输的MAC帧采用四地址结构;

![](https://github.com/ferodevil/virtualization/raw/master/assets/示例1.png)

如图所示，如果STA 1与STA 2通信，STA 1发送三地址结构的MAC帧给AP 2，三个地址字段依次填充AP2、STA 1、STA 2的MAC地址信息（参见表STA 1 -&gt; AP 2），AP 2收到后转发给STA 2，三个地址字段依次修改为STA 2、AP 2、STA 1的MAC地址信息（参见表AP 2 -&gt; STA 2）；如果STA 1与STA 3通信，AP 2收到STA 1的MAC帧后需要转发给AP 1，便将三地址结构修改为四地址结构，四个地址字段依次填充为AP 2、AP 1、STA 3、STA 1的MAC地址信息（参见表AP 2 -&gt; AP 1），AP 1收到后转发给STA 3，又将四地址结构修改为三地址结构。

```
 传输方式         Address 1   Address 2   Address 3   Address 4
 STA 1 -> AP2    RA = AP 2   TA = STA 1  DA = STA 2  N/A
 AP 2 -> STA 2   RA = STA 2  TA = AP 2   SA = STA 1  N/A
 AP 2 -> AP 1    RA = AP 1   TA = AP 2   DA = STA 3  SA = STA 1
```

### 2.  **虚拟网络实现方式**

* **Masquerade alternative**

Linux routing can be used instead with iptables-masquerade and ip \_forward to achieve a bridge but as mentioned this require to enable ip\_forward and will make linux act like a router this need to be setup carefully because it could introduce some security concern.  

```
# bridge setup
brctl addbr br0
ifconfig br0 10.10.20.1/24 up

# enable ipv4 forwarding
echo "1" > /proc/sys/net/ipv4/ip_forward

# netfilter cleanup
iptables --flush
iptables -t nat -F
iptables -X
iptables -Z
iptables -P INPUT ACCEPT
iptables -P OUTPUT ACCEPT
iptables -P FORWARD ACCEPT

# netfilter network address translation
iptables -t nat -A POSTROUTING -o wlan0 -s 10.10.20.0/24  -j MASQUERADE
```

* **Birdge and important**

Also, and very important, you should not use obsolete, deprecated commands like ifconfig, brctl, and so on. The iproute2 suite contains commands for all of this, including setting up virtual interfaces and creating bridges. If you do not know how to set up a bridge with ip, here we go.

```
ip tuntap add tap0 mode tap user root
ip link set tap0 up
ip link add br0 type bridge
ip link set tap0 master br0
ip link set eth0 master br0
ip addr add 10.173.10.1/24  dev br0
ip link set br0 up
```

With this set of commands, we create a virtual interface called tap0, then a bridge called br0, then enslave eth0 and tap0 to the bridge, to which we assign an IP address of 10.173.10.1, then bring it all up. The three separate instances of bringing the interfaces up \(for tap0, eth0, and br0\) are required.

[引用原文](https://serverfault.com/questions/152363/bridging-wlan0-to-eth0)


### 3.  **ipvtap堆栈**
发送数据调用过程：  
```
     0 vhost-1941(1972): -> ipvlan_start_xmit
     0 vhost-1941(1972): -> ipvlan_queue_xmit
     0 vhost-1941(1972): -> ipvlan_get_L3_hdr.isra.21
     0 vhost-1941(1972): -> ipvlan_addr_lookup
     0 vhost-1941(1972): -> ipvlan_ht_addr_lookup
```  

netlink创建ipvtap过程：
```
     0 libvirtd(1069): -> ipvlan_nl_validate
     0 libvirtd(1069): -> ipvtap_setup
     0 libvirtd(1069): -> ipvlan_link_setup
     0 libvirtd(1069): -> ipvtap_newlink
     0 libvirtd(1069): -> ipvlan_link_new
     0 libvirtd(1069): -> ipvlan_init
     0 libvirtd(1069): -> ipvlan_device_event
     0 libvirtd(1069): -> ipvtap_device_event
     0 libvirtd(1069): -> ipvlan_fix_features
     0 libvirtd(1069): -> ipvlan_device_event
     0 libvirtd(1069): -> ipvtap_device_event
     0 libvirtd(1069): -> ipvtap_net_namespace
     0 libvirtd(1069): -> ipvtap_net_namespace
     0 libvirtd(1069): -> ipvlan_device_event
     0 libvirtd(1069): -> ipvtap_device_event
     0 libvirtd(1069): -> ipvlan_device_event
     0 libvirtd(1069): -> ipvtap_device_event
     0 libvirtd(1069): -> ipvlan_set_port_mode
     0 libvirtd(1069): -> ipvlan_nl_getsize
     0 libvirtd(1069): -> ipvlan_get_stats64
     0 libvirtd(1069): -> ipvlan_nl_fillinfo
     0 libvirtd(1069): -> ipvlan_get_iflink
```



